{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": " Cracking the Apple: Predicting Stock Prices",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'apple-stock-price-from-19802021:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1579572%2F3839314%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240129%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240129T160837Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D93e26716e31cf04370650eb0538ce8212672c846bda7f710bcd0d96c94791321c44ff192afdf74545eceb431148584a8986ffb3bdf67711cd07d70411463af9fa9845ba1ff721dc8059ac510e52aa50a2f6b4c0705988d23e1f3549640a1a870d983e1a97784a9636b05a5b964140236620e4678d3453f5a54c87350cd2cd568dfe8e11356db0d71605076f1e51494d23f32b901832d3ee19c03ec2efd6e16f3c380022201d7a1ea068514508900c6454fc05c8c9aed41b0b9ef877edfd159cc42d01ee6f1e00ec3f5979ba40b579056b3b3516532286fd7d0b4d00449a867c2c8b221d2731c65cad836237c994ea2b75ecf4877312beb2e716dca826a0ef9ef'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "bsq5_D3y-uM0"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T19:43:08.837373Z",
          "iopub.execute_input": "2023-05-17T19:43:08.837826Z",
          "iopub.status.idle": "2023-05-17T19:43:08.84672Z",
          "shell.execute_reply.started": "2023-05-17T19:43:08.83779Z",
          "shell.execute_reply": "2023-05-17T19:43:08.845426Z"
        },
        "trusted": true,
        "id": "Zhc5Sitq-uM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " <a id=\"1.2\"></a>\n",
        "###  Load the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "sqTdMKk4-uM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('../input/apple-stock-price-from-19802021/AAPL.csv') # Load Dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T17:42:56.318316Z",
          "iopub.execute_input": "2023-05-17T17:42:56.319158Z",
          "iopub.status.idle": "2023-05-17T17:42:56.375442Z",
          "shell.execute_reply.started": "2023-05-17T17:42:56.319093Z",
          "shell.execute_reply": "2023-05-17T17:42:56.374436Z"
        },
        "trusted": true,
        "id": "SzH1zG2u-uM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " <a id=\"1.3\"></a>\n",
        "###   Data Describtion"
      ],
      "metadata": {
        "id": "H8TCzf_C-uM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T17:42:57.597008Z",
          "iopub.execute_input": "2023-05-17T17:42:57.597456Z",
          "iopub.status.idle": "2023-05-17T17:42:57.664065Z",
          "shell.execute_reply.started": "2023-05-17T17:42:57.597411Z",
          "shell.execute_reply": "2023-05-17T17:42:57.662871Z"
        },
        "trusted": true,
        "id": "Ka5bdpKV-uM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Statistical summary of the data\n",
        "df.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T17:42:57.951038Z",
          "iopub.execute_input": "2023-05-17T17:42:57.951496Z",
          "iopub.status.idle": "2023-05-17T17:42:57.993359Z",
          "shell.execute_reply.started": "2023-05-17T17:42:57.951461Z",
          "shell.execute_reply": "2023-05-17T17:42:57.992065Z"
        },
        "trusted": true,
        "id": "IT0wjilD-uM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some observation from the data statitics:\n",
        "\n",
        "    1. There are 10,468 observations in the dataset.\n",
        "    2. The minimum and maximum values for Open, High, Low, Close, and Adj Close prices are significantly different,   indicating a large range of values for these variables.\n",
        "    3. The standard deviation for each variable is also quite large, indicating a high degree of variability in the data.\n",
        "    4. The volume values in the dataset also have a large range of values, with a mean value of 3.3 billion.\n",
        "   \n",
        "<a id='2'></a>\n",
        "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
        "<p style=\"background-color:#000000;font-family:candaralight;color:#ffffff;font-size:175%;text-align:center;border-radius:10px 10px;\"> DATA PREPROCESSING </p>\n",
        "\n",
        "In this notebook, we will be exploring and analyzing the Apple stock prices dataset. We will start by importing important libraries, loading the data, and giving a brief description of the dataset.\n",
        "\n",
        "<a id=\"2.1\"></a>\n",
        "### Data cleaning\n",
        "    \n",
        "The data set was found to be free of missing values or duplicated rows."
      ],
      "metadata": {
        "id": "ef_eWNGQ-uM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T17:42:58.540814Z",
          "iopub.execute_input": "2023-05-17T17:42:58.54127Z",
          "iopub.status.idle": "2023-05-17T17:42:58.555273Z",
          "shell.execute_reply.started": "2023-05-17T17:42:58.541233Z",
          "shell.execute_reply": "2023-05-17T17:42:58.553985Z"
        },
        "trusted": true,
        "id": "3T7iK1SC-uM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for duplicate rows\n",
        "df.duplicated().sum()\n",
        "\n",
        "# drop duplicate rows\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T17:42:58.742043Z",
          "iopub.execute_input": "2023-05-17T17:42:58.742501Z",
          "iopub.status.idle": "2023-05-17T17:42:58.767831Z",
          "shell.execute_reply.started": "2023-05-17T17:42:58.742464Z",
          "shell.execute_reply": "2023-05-17T17:42:58.76678Z"
        },
        "trusted": true,
        "id": "2aLs5zeH-uM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"2.2\"></a>\n",
        "###  Data transformation\n",
        "    \n",
        "The column data types were examined and the date type was corrected from object to date. and set the 'Date' column as the index"
      ],
      "metadata": {
        "id": "v9Ct_Rp0-uM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the data types of the columns\n",
        "df.dtypes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T17:42:59.190037Z",
          "iopub.execute_input": "2023-05-17T17:42:59.190476Z",
          "iopub.status.idle": "2023-05-17T17:42:59.200177Z",
          "shell.execute_reply.started": "2023-05-17T17:42:59.190443Z",
          "shell.execute_reply": "2023-05-17T17:42:59.198929Z"
        },
        "trusted": true,
        "id": "KMMEYauc-uM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# set the 'Date' column as the index\n",
        "#df.set_index('Date', inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T17:42:59.417873Z",
          "iopub.execute_input": "2023-05-17T17:42:59.418303Z",
          "iopub.status.idle": "2023-05-17T17:42:59.429706Z",
          "shell.execute_reply.started": "2023-05-17T17:42:59.418268Z",
          "shell.execute_reply": "2023-05-17T17:42:59.42822Z"
        },
        "trusted": true,
        "id": "fTCE9OhY-uM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='3'></a>\n",
        "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
        "<p style=\"background-color:#000000;font-family:candaralight;color:#ffffff;font-size:175%;text-align:center;border-radius:10px 10px;\"> FEATURE EMGINEERING </p>\n",
        "\n",
        "In this section, I extracted relevant information from the data and create new features that can potentially improve the performance of my models. By transforming and combining existing features, I aim to provide my models with more meaningful and informative data to make accurate predictions.\n",
        "    \n",
        "<a id=\"3.1\"></a>\n",
        "### Define a function to add new features to the data\n",
        "\n",
        "    \n",
        "I define a function add_features that adds several new features to the data, including day of the week, month, quarter, year, week of the year, day of the year, and lagged features. We then apply this function to the data and drop any rows with missing values. This code is flexible and easy to modify to add or remove features as needed."
      ],
      "metadata": {
        "id": "hJk8e1jc-uM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_features(data):\n",
        "    # Add day of the week feature\n",
        "    data['day_of_week'] = data['Date'].dt.dayofweek\n",
        "\n",
        "    # Add month feature\n",
        "    data['month'] = data['Date'].dt.month\n",
        "\n",
        "    # Add quarter feature\n",
        "    data['quarter'] = data['Date'].dt.quarter\n",
        "\n",
        "    # Add year feature\n",
        "    data['year'] = data['Date'].dt.year\n",
        "\n",
        "    # Add week of the year feature\n",
        "    data['week_of_year'] = data['Date'].dt.isocalendar().week\n",
        "\n",
        "    # Add day of the year feature\n",
        "    data['day_of_year'] = data['Date'].dt.dayofyear\n",
        "\n",
        "    # Add lagged features\n",
        "    data['lag_1'] = data['Close'].shift(1)\n",
        "    data['lag_2'] = data['Close'].shift(2)\n",
        "    data['lag_3'] = data['Close'].shift(3)\n",
        "    data['lag_4'] = data['Close'].shift(4)\n",
        "    data['lag_5'] = data['Close'].shift(5)\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T17:42:59.851603Z",
          "iopub.execute_input": "2023-05-17T17:42:59.852459Z",
          "iopub.status.idle": "2023-05-17T17:42:59.861159Z",
          "shell.execute_reply.started": "2023-05-17T17:42:59.852412Z",
          "shell.execute_reply": "2023-05-17T17:42:59.860104Z"
        },
        "trusted": true,
        "id": "hnXk4IpM-uM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"3.1\"></a>\n",
        "###  Apply the function to the data\n",
        "Apply the function to the stock prices data and dropping any row with missing values\n"
      ],
      "metadata": {
        "id": "IIT3wa-z-uM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to the data\n",
        "df = add_features(df)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T17:43:00.443294Z",
          "iopub.execute_input": "2023-05-17T17:43:00.444479Z",
          "iopub.status.idle": "2023-05-17T17:43:00.507416Z",
          "shell.execute_reply.started": "2023-05-17T17:43:00.444435Z",
          "shell.execute_reply": "2023-05-17T17:43:00.505944Z"
        },
        "trusted": true,
        "id": "l6OD5P2Q-uM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4'></a>\n",
        "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
        "<p style=\"background-color:#000000;font-family:candaralight;color:#ffffff;font-size:175%;text-align:center;border-radius:10px 10px;\"> EXPLORATORY DATA ANALYSIS </p>\n",
        "\n",
        "This section explores the dataset using various visualizations such as distribution plots, line plots, heatmaps, and pairplots to gain insights into the relationships between features and the target variable.\n",
        "    "
      ],
      "metadata": {
        "id": "2Fo3GEMW-uM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Distribution of the target variable\n",
        "sns.displot(data=df, x='Close', kde=True)\n",
        "plt.title('Distribution of Close Prices')\n",
        "plt.show()\n",
        "\n",
        "# Line plot of close prices over time\n",
        "sns.lineplot(data=df, x='Date', y='Close')\n",
        "plt.title('Close Prices Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Heatmap of correlation between features\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, cmap='coolwarm', annot=True)\n",
        "plt.title('Correlation Between Features')\n",
        "plt.show()\n",
        "\n",
        "# Pairplot of features\n",
        "\n",
        "sns.pairplot(data=df, vars=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
        "plt.title('Pairplot of Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T17:43:02.312287Z",
          "iopub.execute_input": "2023-05-17T17:43:02.313165Z",
          "iopub.status.idle": "2023-05-17T17:43:16.622332Z",
          "shell.execute_reply.started": "2023-05-17T17:43:02.313087Z",
          "shell.execute_reply": "2023-05-17T17:43:16.621023Z"
        },
        "trusted": true,
        "id": "M8UlrhvA-uM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation\n",
        "\n",
        "    1. Distribution plot of Close Prices:\n",
        "\n",
        "*  Shows the distribution of Close Prices\n",
        "*  Indicates that the majority of the Close Prices are in the range of 0 to 35\n",
        "*  Significant skewed to the right\n",
        "\n",
        "    2. Line plot of Close Prices over time:\n",
        "    \n",
        "\n",
        "*  Shows how the Close Prices have changed over time\n",
        "*  Indicates that there has been significant fluctuation in the Close Prices, with a few major peaks and troughs from 2005\n",
        "*  There is a huge uptrend since 2015 and still going.\n",
        "\n",
        "    3. Heatmap of correlation between features:\n",
        "    \n",
        "\n",
        "*  Shows the correlation between all the features in the dataset\n",
        "*  Indicates that the Close Price has a strong positive correlation with Open, High, Low, and Adj Close prices, but a weak negative correlation with Volume\n",
        "*  Also indicates that the Open, High, Low, and Adj Close prices are highly correlated with each other\n",
        "\n",
        "    4. Pairplot of features:\n",
        "    \n",
        "\n",
        "* Shows the pairwise relationships between all the features in the dataset\n",
        "\n",
        "* Indicates that the Open, High, Low, and Close prices are strongly positively correlated with each other, with a linear relationship\n",
        "\n",
        "* Also indicates that the Volume feature is not strongly correlated with any of the other features"
      ],
      "metadata": {
        "id": "WNUODRGr-uM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='5'></a>\n",
        "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
        "<p style=\"background-color:#000000;font-family:candaralight;color:#ffffff;font-size:175%;text-align:center;border-radius:10px 10px;\"> MODELING </p>\n",
        "\n",
        "The modeling section is where I explore different algorithms to predict the stock prices. I start with a baseline model and gradually move towards more complex models like Linear Regression, Support Vector Regression, Random Forest Regression, and LSTM. Each model is evaluated for its performance and compared with the others to choose the best one.\n",
        "    \n",
        "<a id='5.1'></a>\n",
        "###  Baseline model\n",
        "\n",
        "Building a baseline model is good start to compare the performance of other models against."
      ],
      "metadata": {
        "id": "aJ--Djtw-uM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['Open', 'High', 'Low', 'Volume']], df['Close'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Baseline model\n",
        "y_pred_baseline = np.full((len(y_test),), y_train.mean())\n",
        "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
        "rmse_baseline = np.sqrt(mse_baseline)\n",
        "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
        "mape_baseline = np.mean(np.abs((y_test - y_pred_baseline) / y_test)) * 100\n",
        "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
        "\n",
        "print('Baseline Model:')\n",
        "print(f'MSE: {mse_baseline:.2f}')\n",
        "print(f'RMSE: {rmse_baseline:.2f}')\n",
        "print(f'MAE: {mae_baseline:.2f}')\n",
        "print(f'MAPE: {mape_baseline:.2f}%')\n",
        "print(f'R2 Score: {r2_baseline:.2f}\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T18:20:14.705187Z",
          "iopub.execute_input": "2023-05-17T18:20:14.705897Z",
          "iopub.status.idle": "2023-05-17T18:20:14.723705Z",
          "shell.execute_reply.started": "2023-05-17T18:20:14.705859Z",
          "shell.execute_reply": "2023-05-17T18:20:14.722697Z"
        },
        "trusted": true,
        "id": "wpLIJ9Wz-uM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What the model tell us:\n",
        "\n",
        "    I would say that the baseline model is not performing well. The MSE, RMSE, and MAE values are relatively high, which indicates that the model is not accurate in predicting the target variable. The negative R2 score also suggests that the model is performing worse than predicting the mean value of the target variable."
      ],
      "metadata": {
        "id": "o203z5il-uM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='5.2'></a>\n",
        "###  Linear Regression Model\n"
      ],
      "metadata": {
        "id": "Bei7EplJ-uM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression Model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "rmse_lr = np.sqrt(mse_lr)\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "mape_lr = np.mean(np.abs((y_test - y_pred_lr) / y_test)) * 100\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "print('Linear Regression Model:')\n",
        "print(f'MSE: {mse_lr:.2f}')\n",
        "print(f'RMSE: {rmse_lr:.2f}')\n",
        "print(f'MAE: {mae_lr:.2f}')\n",
        "print(f'MAPE: {mape_lr:.2f}%')\n",
        "print(f'R2 Score: {r2_lr:.2f}\\n')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T18:20:18.699872Z",
          "iopub.execute_input": "2023-05-17T18:20:18.70028Z",
          "iopub.status.idle": "2023-05-17T18:20:18.731994Z",
          "shell.execute_reply.started": "2023-05-17T18:20:18.700248Z",
          "shell.execute_reply": "2023-05-17T18:20:18.730702Z"
        },
        "trusted": true,
        "id": "K1sOZ9Nh-uM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What the model tell us:\n",
        "\n",
        "    The Linear Regression model is performing well, with a very low error and an R2 score of 1 indicating a perfect fit. However, it is important to note that overfitting is possible and further evaluation may be necessary."
      ],
      "metadata": {
        "id": "mC5ibDv9-uM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='5.3'></a>\n",
        "###  Support Vector Regression Model\n"
      ],
      "metadata": {
        "id": "U-GRHy9U-uM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Regression Model\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svr_model = SVR(kernel='linear')\n",
        "svr_model.fit(X_train_scaled, y_train)\n",
        "y_pred_svr = svr_model.predict(X_test_scaled)\n",
        "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
        "rmse_svr = np.sqrt(mse_svr)\n",
        "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
        "mape_svr = np.mean(np.abs((y_test - y_pred_svr) / y_test)) * 100\n",
        "r2_svr = r2_score(y_test, y_pred_svr)\n",
        "\n",
        "print('Support Vector Regression Model:')\n",
        "print(f'MSE: {mse_svr:.2f}')\n",
        "print(f'RMSE: {rmse_svr:.2f}')\n",
        "print(f'MAE: {mae_svr:.2f}')\n",
        "print(f'MAPE: {mape_svr:.2f}%')\n",
        "print(f'R2 Score: {r2_svr:.2f}\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T18:20:25.155172Z",
          "iopub.execute_input": "2023-05-17T18:20:25.155615Z",
          "iopub.status.idle": "2023-05-17T18:20:25.867359Z",
          "shell.execute_reply.started": "2023-05-17T18:20:25.155578Z",
          "shell.execute_reply": "2023-05-17T18:20:25.866109Z"
        },
        "trusted": true,
        "id": "kYZW-AX1-uM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does this model tell us:\n",
        "\n",
        "- The SVR model is fit to the training data using a linear kernel.\n",
        "\n",
        "- Based on the evaluation, the model has an RMSE of 0.40, which means that, on average, the predicted values are off by about 0.40 units.\n",
        "\n",
        "- The model has an MAPE of 20.31%, which means that, on average, the predicted values are off by about 20.31% of the true values.\n",
        "\n",
        "- The R2 score of 1.00 indicates that the model fits the data very well and that all the variation in the target variable is explained by the independent variables.\n",
        "\n",
        "- The performance of the model can be improved by tuning hyperparameters, trying different kernels, or using different feature engineering techniques."
      ],
      "metadata": {
        "id": "mx5z1sHC-uM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='5.4'></a>\n",
        "### Random Forest Regression Model\n"
      ],
      "metadata": {
        "id": "jHzb_v_B-uM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Regression Model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "mape_rf = np.mean(np.abs((y_test - y_pred_rf) / y_test)) * 100\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print('Random Forest Regression Model:')\n",
        "print(f'MSE: {mse_rf:.2f}')\n",
        "print(f'RMSE: {rmse_rf:.2f}')\n",
        "print(f'MAE: {mae_rf:.2f}')\n",
        "print(f'MAPE: {mape_rf:.2f}%')\n",
        "print(f'R2 Score: {r2_rf:.2f}\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T18:20:28.701835Z",
          "iopub.execute_input": "2023-05-17T18:20:28.704304Z",
          "iopub.status.idle": "2023-05-17T18:20:32.076453Z",
          "shell.execute_reply.started": "2023-05-17T18:20:28.70426Z",
          "shell.execute_reply": "2023-05-17T18:20:32.07521Z"
        },
        "trusted": true,
        "id": "oUtIbSY2-uM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does this model tell us:\n",
        "\n",
        "-  The Random Forest Regression Model has a relatively low MSE, RMSE, MAE, and MAPE compared to the Baseline Model and the Support Vector Regression Model.\n",
        "-  The R2 Score is 1.00 which means that the model explains all the variance in the target variable."
      ],
      "metadata": {
        "id": "YepxT92S-uM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='5.5'></a>\n",
        "### LSTM Model\n"
      ],
      "metadata": {
        "id": "Ye6nfEI3-uM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T19:43:30.127808Z",
          "iopub.execute_input": "2023-05-17T19:43:30.128274Z",
          "iopub.status.idle": "2023-05-17T19:43:30.134765Z",
          "shell.execute_reply.started": "2023-05-17T19:43:30.128235Z",
          "shell.execute_reply": "2023-05-17T19:43:30.133062Z"
        },
        "trusted": true,
        "id": "XG3QvMwP-uM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataframe with only the 'Close column\n",
        "data = df.filter(['Close'])\n",
        "# Convert the dataframe to a numpy array\n",
        "dataset = data.values\n",
        "# Get the number of rows to train the model on\n",
        "len_train_data = int(np.ceil( len(dataset) * .95 ))\n",
        "\n",
        "len_train_data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T19:44:10.104729Z",
          "iopub.execute_input": "2023-05-17T19:44:10.105231Z",
          "iopub.status.idle": "2023-05-17T19:44:10.116464Z",
          "shell.execute_reply.started": "2023-05-17T19:44:10.10519Z",
          "shell.execute_reply": "2023-05-17T19:44:10.1151Z"
        },
        "trusted": true,
        "id": "kFDuJWYJ-uM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "trained_scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "trained_scaled_data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T19:45:21.52996Z",
          "iopub.execute_input": "2023-05-17T19:45:21.530433Z",
          "iopub.status.idle": "2023-05-17T19:45:21.542777Z",
          "shell.execute_reply.started": "2023-05-17T19:45:21.530398Z",
          "shell.execute_reply": "2023-05-17T19:45:21.541298Z"
        },
        "trusted": true,
        "id": "flOtZrMk-uM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training data set\n",
        "# Create the scaled training data set\n",
        "train_data = trained_scaled_data[0:int(len_train_data), :]\n",
        "# Split the data into x_train and y_train data sets\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(60, len(train_data)):\n",
        "    x_train.append(train_data[i-60:i, 0])\n",
        "    y_train.append(train_data[i, 0])\n",
        "    if i<= 61:\n",
        "        print(x_train)\n",
        "        print(y_train)\n",
        "        print()\n",
        "\n",
        "# Convert the x_train and y_train to numpy arrays\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "\n",
        "# Reshape the data\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "# x_train.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T19:45:36.28163Z",
          "iopub.execute_input": "2023-05-17T19:45:36.282093Z",
          "iopub.status.idle": "2023-05-17T19:45:36.322567Z",
          "shell.execute_reply.started": "2023-05-17T19:45:36.282055Z",
          "shell.execute_reply": "2023-05-17T19:45:36.321285Z"
        },
        "trusted": true,
        "id": "33Rvtbhi-uM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n",
        "model.add(LSTM(64, return_sequences=False))\n",
        "model.add(Dense(25))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=14)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T19:46:02.783842Z",
          "iopub.execute_input": "2023-05-17T19:46:02.784344Z",
          "iopub.status.idle": "2023-05-17T19:53:40.216822Z",
          "shell.execute_reply.started": "2023-05-17T19:46:02.784305Z",
          "shell.execute_reply": "2023-05-17T19:53:40.215851Z"
        },
        "trusted": true,
        "id": "j8c3eq8y-uM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the testing data set\n",
        "# Create a new array containing scaled values from index 1543 to 2002\n",
        "test_data = trained_scaled_data[len_train_data - 60: , :]\n",
        "# Create the data sets x_test and y_test\n",
        "x_test = []\n",
        "y_test = dataset[len_train_data:, :]\n",
        "for i in range(60, len(test_data)):\n",
        "    x_test.append(test_data[i-60:i, 0])\n",
        "\n",
        "# Convert the data to a numpy array\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "# Reshape the data\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
        "\n",
        "# Get the models predicted price values\n",
        "predictions = model.predict(x_test)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "# Get the root mean squared error (RMSE)\n",
        "rmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))\n",
        "rmse\n",
        "# Baseline model\n",
        "mse_lstm = mean_squared_error(y_test, predictions)\n",
        "rmse_lstm = np.sqrt(mse_lstm)\n",
        "mae_lstm = mean_absolute_error(y_test, predictions)\n",
        "mape_lstm = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
        "r2_lstm = r2_score(y_test, predictions)\n",
        "\n",
        "print('lstm Model:')\n",
        "print(f'MSE: {mse_lstm:.2f}')\n",
        "print(f'RMSE: {rmse_lstm:.2f}')\n",
        "print(f'MAE: {mae_lstm:.2f}')\n",
        "print(f'MAPE: {mape_lstm:.2f}%')\n",
        "print(f'R2 Score: {r2_lstm:.2f}\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T19:54:48.59246Z",
          "iopub.execute_input": "2023-05-17T19:54:48.592917Z",
          "iopub.status.idle": "2023-05-17T19:54:50.677645Z",
          "shell.execute_reply.started": "2023-05-17T19:54:48.592879Z",
          "shell.execute_reply": "2023-05-17T19:54:50.676572Z"
        },
        "trusted": true,
        "id": "_oG53CQM-uNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(y_test, color = 'red', label = 'Real Stock Price')\n",
        "plt.plot(predictions, color = 'green', label = 'Predicted Stock Price')\n",
        "plt.title(' Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel(' Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T19:54:53.719977Z",
          "iopub.execute_input": "2023-05-17T19:54:53.721256Z",
          "iopub.status.idle": "2023-05-17T19:54:54.00848Z",
          "shell.execute_reply.started": "2023-05-17T19:54:53.721201Z",
          "shell.execute_reply": "2023-05-17T19:54:54.007188Z"
        },
        "trusted": true,
        "id": "UcUIS62b-uNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LSTM model shows promising performance\n",
        "\n",
        "1. Low prediction errors (MSE and RMSE) and relatively small deviations from the actual values (MAE).\n",
        "\n",
        "2. The MAPE indicates a low average percentage difference, suggesting accurate predictions in relative terms.\n",
        "\n",
        "3. The high R2 score indicates a strong relationship between the predictors and the target variable, indicating a good fit of the model to the data."
      ],
      "metadata": {
        "id": "MnE1VYG0-uNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='6'></a>\n",
        "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
        "<p style=\"background-color:#000000;font-family:candaralight;color:#ffffff;font-size:175%;text-align:center;border-radius:10px 10px;\"> Model Comparison </p>\n",
        "\n",
        "By comparing the results of different models, we can determine which approach yields the best performance for our specific problem. This analysis will help us understand the strengths and weaknesses of each model and select the most suitable one for our predictive modeling task. Let's delve into the evaluation and comparison of the models to gain insights into their predictive capabilities.\n",
        "     "
      ],
      "metadata": {
        "id": "0-oaAJv4-uNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['Baseline', 'Linear Regression', 'Support Vector Regression', 'Random Forest Regression', 'LSTM']\n",
        "mse_scores = [mse_baseline, mse_lr, mse_svr, mse_rf, mse_lstm]\n",
        "rmse_scores = [rmse_baseline, rmse_lr, rmse_svr, rmse_rf, rmse_lstm]\n",
        "mae_scores = [mae_baseline, mae_lr, mae_svr, mae_rf, mae_lstm]\n",
        "mape_scores = [mape_baseline, mape_lr, mape_svr, mape_rf, mape_lstm]\n",
        "r2_scores = [r2_baseline, r2_lr, r2_svr, r2_rf, r2_lstm]\n",
        "\n",
        "# Create a dataframe to store the evaluation metrics\n",
        "evaluation_df = pd.DataFrame({'Model': models, 'MSE': mse_scores, 'RMSE': rmse_scores, 'MAE': mae_scores, 'MAPE': mape_scores, 'R2 Score': r2_scores})\n",
        "evaluation_df.set_index('Model', inplace=True)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(evaluation_df)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-17T19:05:54.264715Z",
          "iopub.execute_input": "2023-05-17T19:05:54.265211Z",
          "iopub.status.idle": "2023-05-17T19:05:54.282188Z",
          "shell.execute_reply.started": "2023-05-17T19:05:54.265171Z",
          "shell.execute_reply": "2023-05-17T19:05:54.280904Z"
        },
        "trusted": true,
        "id": "jskC0EtV-uNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table presents the evaluation metrics for different models:\n",
        "\n",
        "**1. Baseline:** This model has a high MSE of 965.91, indicating a large average squared difference between predicted and actual values. The RMSE of 31.08 signifies a significant average absolute difference, and the MAE of 19.45 represents a substantial average absolute error. The MAPE of 4091.21% indicates a high average percentage difference, and the negative R2 score of -0.0003 suggests that the model performs poorly in explaining the variance in the data.\n",
        "\n",
        "**2. Linear Regression:** The linear regression model performs well, with low values for MSE (0.062), RMSE (0.249), MAE (0.072), and MAPE (0.813%). The high R2 score of 0.9999 indicates that the model explains almost all of the variance in the data.\n",
        "\n",
        "**3. Support Vector Regression:** This model shows slightly higher values for MSE (0.165), RMSE (0.406), MAE (0.157), and MAPE (21.41%). The R2 score of 0.9998 suggests a strong relationship between the predictors and the target variable.\n",
        "\n",
        "**4. Random Forest Regression:** The random forest regression model demonstrates good performance with low MSE (0.130), RMSE (0.361), MAE (0.100), and MAPE (0.867%) values. The R2 score of 0.9999 indicates a high degree of variance explained by the model.\n",
        "\n",
        "**5. LSTM:** The LSTM model performs reasonably well, with an MSE of 12.414, RMSE of 3.523, MAE of 2.742, and MAPE of 1.995%. The R2 score of 0.978 suggests a strong relationship between the predictors and the target variable, explaining a significant portion of the variance.\n",
        "\n",
        "Overall, the linear regression, support vector regression, random forest regression, and LSTM models outperform the baseline model in terms of prediction accuracy, with the LSTM model exhibiting good performance across multiple evaluation metrics."
      ],
      "metadata": {
        "id": "ooD9D47h-uNH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzASi5at-uNH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}